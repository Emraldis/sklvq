@article{Villmann2019,
abstract = {An appropriate choice of the activation function (like ReLU, sigmoid or swish) plays an important role in the performance of (deep) multilayer perceptrons (MLP) for classification and regression learning. Prototype-based classification learning methods like (generalized) learning vector quantization (GLVQ) are powerful alternatives. These models also deal with activation functions but here they are applied to the so-called classifier function instead. In this paper we investigate successful candidates of activation functions known for MLPs for application in GLVQ and their influence on the performance.},
journal = {arXiv},

author = {Villmann, Thomas and Ravichandran, John and Villmann, Andrea and Nebel, David and Kaden, Marika},

number = {January},
title = {{Activation Functions for Generalized Learning Vector Quantization - A Performance Comparison}},
url = {http://arxiv.org/abs/1901.05995},
year = {2019}
}

@article{Sato1996,
abstract = {We propose a new learning method, "Generalized Learning Vec-tor Quantization (GLVQ)," in which reference vectors are updated based on the steepest descent method in order to minimize the cost function . The cost function is determined so that the obtained learning rule satisfies the convergence condition. We prove that Kohonen's rule as used in LVQ does not satisfy the convergence condition and thus degrades recognition ability. Experimental re-sults for printed Chinese character recognition reveal that GLVQ is superior to LVQ in recognition ability.},
author = {Sato, A. and Yamada, K.},
file = {:Users/rick/OneDrive/Documents/Acrossing/ESR11/Literature/Processed/Sato, Yamada - 1996 - Generalized Learning Vector Quantization(2).pdf:pdf},
journal = {Advances in Neural Network Information Processing Systems},
mendeley-groups = {Learning Vector Quantization},
pages = {423--429},
title = {{Generalized Learning Vector Quantization}},
url = {http://papers.nips.cc/paper/1113-generalized-learning-vector-quantization},
year = {1996}
}
